## 2、朴素贝叶斯

### 原理

​	依靠最大似然估计来反推参数C（其实就是种类）。

### 方法

​	拉普拉斯平滑：

​		是为了解决$P(x_{ij} | y_i)$不存在某类别之中的问题，也就是可能某特征的某一个取值全部没有出现在某类$y_i$中.

​	EM算法：

​		是为了解决$P(x_{ij} | y)$不存在训练集之中的问题，也就是可能某特征的某一个取值没有出现在训练集当中.

### 垃圾邮件处理

​	垃圾邮件处理的核心（我目前能达到的核心）是对一个词出现与否进行判断，将一个词在一个种类中出现与否各确定一个概率，之后对测试集进行拟合，整体的思路可以简化朴素贝叶斯中对每一个属性二值化的二分类问题。在训练集当中我们只关注一个词出现与否，但是在测试集当中，如果一个词出现的次数多于一次，我们就对那个词**已经出现的概率**取在测试集中出现次数$n$的$n$次方，这样的处理主要是为了避免出现EM算法当中隐变量出现的问题，虽然这样的处理必然导致准确性的降低，但是在实战当中仍然能够取得不错的识别效果，说明了贝叶斯在文字处理方面出彩的能力。

​	模型在处理训练集中完全没出现过的词的时候能力非常薄弱，只能直接略过。

### 杂记

​	运用上述的处理思路，分别跑了三个数据集：

​		（1）培训提供的垃圾邮件数据集（50样本，600词），准确率随着训练集与测试集的分割而不同，最低80最高可达100；

​		（2）$sklearn$邮件数据集（2000样本，50000词，四分类），准确率64%，比较惨淡了，而且跑的时间接近半小时；

​					二分类的话效果会稍微好一点，能够达到80左右，说明模型在二分类问题上可能比较有优势一点。

​		（3）乳腺癌数据集，分箱后将多取值的特征当成二值特征进行分类（这里分箱直接分两箱的话就比较合理），结果居然能达到95%左右，出乎意料。

