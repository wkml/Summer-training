# 集成学习

## Boosting（串行生成）

​	工作机制：先从初始训练集训练出一个基学习器，再根据基学习器的表现对样本分布进行调整，**使得先前的基学习器做错的训练样本在后续收到更多的关注**，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到实现指定的值T，或整个集成结果达到退出条件，然后**将这些学习器进行加权结合**。

​	总结：多个分类器，寻找一个combiner。

### AdaBoost

​	Adaboost 算法采用**调整样本权重**的方式来对样本分布进行调整，即**提高前一轮个体学习器错误分类的样本的权重**，而降低那些正确分类的样本的权重，这样就能使得错误分类的样本可以受到更多的关注，从而在下一轮中可以正确分类，使得分类问题被一系列的弱分类器“分而治之”。对于组合方式，AdaBoost采用加权多数表决的方法，具体地，加大分类误差率小的若分类器的权值，减小分类误差率大的若分类器的权值，从而调整他们在表决中的作用。

​	具体过程如下：

​	训练数据中的每一个样本，并赋予其一贯权重，这些权重构成了向量 D $ D$。一开始，这些权重都被初始化成相同的值。首先在训练数据上训练出一个弱分类器并计算该分类器的错误率，然后在同一数据集上再次训练弱分类器。在分类器的第二次训练中，将会重新调整每个样本的权重，其中第一次分对的样本的权重将会降低，而第一次分错的样本的权重将会升高。为了从所有弱分类器中得到最终的分类结果，AdaBoost为每个分类器都分配了一个权重 $\alpha$ ，这些$\alpha$是基于每个弱分类器的错误率计算的。

### BDT（提升树）

​	提升树(boosting tree)。对于二类分类的算法，决策树内部为CART分类树，此时就为AdaBoost基学习器为CART分类树的特殊情况；对于回归问题，提升树内部决策树是二叉回归树，在每次迭代中通过拟合当前模型的残差生成一个新的回归树，并与之前的弱学习器结合生成新的强学习器，若最后生成的强学习器满足误差要求，则为所求的提升树。（与AdaBoost回归的区别在AdaBoost每次迭代通过分类误差率更新样本的权重，并计算alpha值，最终取中位数的alpha对应的弱学习器为强学习器）

#### GDBT（梯度提升树）

​	在分类问题上，退化为Adaboost，应用于回归问题。

#### XGBoost

​	GDBT的一种优化。
$$
XGBoost = eXtreme + GBDT
$$





## Bagging（并行生成）

​	有放回采样的选择样本，然后所有模型进行投票。

### RF

​	基本的策略是每次随机选择n个特征进行建树。